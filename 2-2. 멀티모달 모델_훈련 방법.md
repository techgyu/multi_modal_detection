# Attention-based Multi-modal YOLO ÌõàÎ†® Í≥ÑÌöç (ÏµúÏ¢Ö)

**Î™©Ìëú**: Ïó¥ÏÉÅ + ÏïºÍ∞ÑÌà¨Ïãú + Enhanced Í∞ÄÏãúÍ¥ë 3-way ÏûÖÎ†•ÏúºÎ°ú ÏïºÍ∞Ñ Person ÌÉêÏßÄ  
**ÏïÑÌÇ§ÌÖçÏ≤ò**: Attention-based Late Fusion  
**ÌïòÎìúÏõ®Ïñ¥**: RTX 4090 24GB, Ryzen 7 7800X, 32GB DDR5

---

## üìã Ï†ÑÏ≤¥ ÌõàÎ†® Î°úÎìúÎßµ

```
Phase 0: ÌôòÍ≤Ω ÏÑ§Ï†ï Î∞è Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
    ‚Üì
Phase 1: Ïó¥ÏÉÅ Îã®Ïùº Î™®Îã¨ Baseline Î™®Îç∏ ÌõàÎ†® (Plan C)
    ‚Üì
Phase 2: Attention Fusion 3-way Î™®Îç∏ Íµ¨ÌòÑ
    ‚Üì
Phase 3: ÏÇ¨Ï†ÑÌïôÏäµ Í∞ÄÏ§ëÏπò Î°úÎìú Î∞è Fine-tuning
    ‚Üì
Phase 4: ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏµúÏ†ÅÌôî
    ‚Üì
Phase 5: ÏµúÏ¢Ö ÌèâÍ∞Ä Î∞è Î∞∞Ìè¨
```

---

# Phase 0: ÌôòÍ≤Ω ÏÑ§Ï†ï Î∞è Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ

## 0-1. Í∞úÎ∞ú ÌôòÍ≤Ω Íµ¨Ï∂ï

### Python ÌôòÍ≤Ω
```bash
# Conda Í∞ÄÏÉÅÌôòÍ≤Ω ÏÉùÏÑ± (Python 3.10 Í∂åÏû•)
conda create -n multimodal_detection python=3.10
conda activate multimodal_detection
```

### ÌïÑÏàò Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò
```bash
# PyTorch (CUDA 12.1 Í∏∞Ï§Ä)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Ultralytics YOLO
pip install ultralytics

# Ï∂îÍ∞Ä ÎùºÏù¥Î∏åÎü¨Î¶¨
pip install albumentations opencv-python pillow matplotlib tensorboard
pip install pandas numpy scikit-learn tqdm
pip install timm einops  # Attention Î™®ÎìàÏö©
```

### ÌîÑÎ°úÏ†ùÌä∏ ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞
```
multi_modal_detection/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ daytime/          # Ï£ºÍ∞Ñ Îç∞Ïù¥ÌÑ∞
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ thermal/      # Ïó¥ÏÉÅ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ visual/       # Í∞ÄÏãúÍ¥ë
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ labels/       # YOLO ÌòïÏãù ÎùºÎ≤®
‚îÇ   ‚îú‚îÄ‚îÄ nighttime/        # ÏïºÍ∞Ñ Îç∞Ïù¥ÌÑ∞
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ thermal/      # Ïó¥ÏÉÅ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nvg/          # ÏïºÍ∞ÑÌà¨Ïãú
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ visual/       # ÏõêÎ≥∏ Í∞ÄÏãúÍ¥ë
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ visual_enhanced/ # Enhanced Í∞ÄÏãúÍ¥ë
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ labels/       # YOLO ÌòïÏãù ÎùºÎ≤®
‚îÇ   ‚îî‚îÄ‚îÄ yaml/
‚îÇ       ‚îú‚îÄ‚îÄ thermal.yaml      # Phase 1Ïö©
‚îÇ       ‚îî‚îÄ‚îÄ multimodal.yaml   # Phase 3Ïö©
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ attention_fusion.py   # Attention Î™®Îìà
‚îÇ   ‚îú‚îÄ‚îÄ multimodal_yolo.py    # Î©îÏù∏ Î™®Îç∏
‚îÇ   ‚îî‚îÄ‚îÄ data_loader.py        # Îç∞Ïù¥ÌÑ∞ Î°úÎçî
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ phase1_thermal.py     # Phase 1 ÌõàÎ†®
‚îÇ   ‚îú‚îÄ‚îÄ phase3_multimodal.py  # Phase 3 ÌõàÎ†®
‚îÇ   ‚îî‚îÄ‚îÄ evaluate.py           # ÌèâÍ∞Ä
‚îú‚îÄ‚îÄ weights/                  # ÌïôÏäµÎêú Í∞ÄÏ§ëÏπò Ï†ÄÏû•
‚îú‚îÄ‚îÄ runs/                     # TensorBoard Î°úÍ∑∏
‚îî‚îÄ‚îÄ configs/
    ‚îî‚îÄ‚îÄ hyperparams.yaml      # ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞
```

## 0-2. Îç∞Ïù¥ÌÑ∞ÏÖã Ï§ÄÎπÑ

### YOLO ÌòïÏãù ÎùºÎ≤® Íµ¨Ï°∞
```
# labels/image001.txt
# class_id x_center y_center width height (Ï†ïÍ∑úÌôî 0~1)
0 0.512 0.348 0.124 0.256
0 0.723 0.512 0.098 0.187
```

### Îç∞Ïù¥ÌÑ∞ÏÖã YAML ÌååÏùº ÏûëÏÑ±

**thermal.yaml** (Phase 1Ïö©):
```yaml
# data/yaml/thermal.yaml
path: ../data/daytime  # Îç∞Ïù¥ÌÑ∞ÏÖã Î£®Ìä∏
train: thermal/train   # ÌïôÏäµ Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî
val: thermal/val       # Í≤ÄÏ¶ù Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî
test: thermal/test     # ÌÖåÏä§Ìä∏ Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî

# Classes
nc: 1  # ÌÅ¥ÎûòÏä§ Í∞úÏàò
names: ['person']  # ÌÅ¥ÎûòÏä§ Ïù¥Î¶Ñ
```

**multimodal.yaml** (Phase 3Ïö©):
```yaml
# data/yaml/multimodal.yaml
path: ../data/nighttime
train: train
val: val
test: test

# Modality paths (custom field)
thermal_path: thermal
nvg_path: nvg
visual_enhanced_path: visual_enhanced

nc: 1
names: ['person']
```

### Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï† ÎπÑÏú®
```python
# Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†
Train: 70% (ÌïôÏäµ)
Val:   15% (Í≤ÄÏ¶ù, ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù)
Test:  15% (ÏµúÏ¢Ö ÌèâÍ∞Ä)

# ÏòàÏãú: 1000Ïû• ÏàòÏßë Ïãú
Train: 700Ïû•
Val:   150Ïû•
Test:  150Ïû•
```

---

# Phase 1: Ïó¥ÏÉÅ Îã®Ïùº Î™®Îã¨ Baseline Î™®Îç∏ ÌõàÎ†®

**Î™©Ï†Å**: Ïó¥ÏÉÅÎßåÏúºÎ°ú Person ÌÉêÏßÄ Îä•Î†• ÌôïÎ≥¥ (Phase 3Ïùò ÏÇ¨Ï†ÑÌïôÏäµ Í∞ÄÏ§ëÏπòÎ°ú ÏÇ¨Ïö©)

## 1-1. ÌõàÎ†® Ïä§ÌÅ¨Î¶ΩÌä∏

**scripts/phase1_thermal.py**:
```python
from ultralytics import YOLO
import torch

def train_thermal_baseline():
    """Phase 1: Ïó¥ÏÉÅ Îã®Ïùº Î™®Îã¨ Î™®Îç∏ ÌõàÎ†®"""
    
    # YOLO11x Î™®Îç∏ Î°úÎìú (COCO ÏÇ¨Ï†ÑÌïôÏäµ Í∞ÄÏ§ëÏπò)
    model = YOLO('yolo11x.pt')
    
    # ÌõàÎ†® ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞
    results = model.train(
        # Îç∞Ïù¥ÌÑ∞
        data='data/yaml/thermal.yaml',
        
        # Î™®Îç∏ ÏÑ§Ï†ï
        imgsz=640,              # ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞
        
        # ÌõàÎ†® ÌååÎùºÎØ∏ÌÑ∞
        epochs=100,             # ÏóêÌè¨ÌÅ¨ Ïàò
        batch=16,               # Î∞∞Ïπò ÏÇ¨Ïù¥Ï¶à (4090 24GB Í∏∞Ï§Ä)
        
        # Optimizer
        optimizer='AdamW',      # Adam with weight decay
        lr0=0.001,              # Ï¥àÍ∏∞ ÌïôÏäµÎ•†
        lrf=0.01,               # ÏµúÏ¢Ö ÌïôÏäµÎ•† (lr0 * lrf)
        momentum=0.937,
        weight_decay=0.0005,
        
        # Scheduler
        cos_lr=True,            # Cosine annealing LR scheduler
        
        # Augmentation
        hsv_h=0.015,            # ÏÉâÏ°∞ Î≥ÄÌôî (Ïó¥ÏÉÅÏùÄ Îã®ÏÉâÏù¥ÎØÄÎ°ú ÏûëÍ≤å)
        hsv_s=0.4,              # Ï±ÑÎèÑ
        hsv_v=0.4,              # Î™ÖÎèÑ
        degrees=10.0,           # ÌöåÏ†Ñ Í∞ÅÎèÑ
        translate=0.1,          # Ïù¥Îèô
        scale=0.5,              # Ïä§ÏºÄÏùº
        shear=0.0,              # Ï†ÑÎã® Î≥ÄÌòï
        perspective=0.0001,     # ÏõêÍ∑º Î≥ÄÌòï
        flipud=0.0,             # ÏÉÅÌïò Î∞òÏ†Ñ (ÏÇ¨ÎûåÏùÄ Î≥¥ÌÜµ Ïïà Ìï®)
        fliplr=0.5,             # Ï¢åÏö∞ Î∞òÏ†Ñ
        mosaic=1.0,             # Mosaic augmentation
        mixup=0.1,              # Mixup augmentation
        
        # Ï†ïÍ∑úÌôî
        dropout=0.0,            # Dropout (YOLOÎäî Î≥¥ÌÜµ ÏÇ¨Ïö© Ïïà Ìï®)
        
        # ÏÑ±Îä• ÏµúÏ†ÅÌôî
        amp=True,               # Automatic Mixed Precision (FP16)
        workers=8,              # Îç∞Ïù¥ÌÑ∞ Î°úÎçî ÏõåÏª§ (CPU ÏΩîÏñ¥ Ïàò)
        device=0,               # GPU 0Î≤à ÏÇ¨Ïö©
        
        # Ï†ÄÏû• Î∞è Î°úÍπÖ
        project='runs/phase1',
        name='thermal_baseline',
        exist_ok=True,
        save=True,
        save_period=10,         # 10 ÏóêÌè¨ÌÅ¨ÎßàÎã§ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû•
        
        # Validation
        val=True,
        plots=True,             # ÌõàÎ†® ÌîåÎ°Ø ÏÉùÏÑ±
        
        # Early stopping
        patience=50,            # 50 ÏóêÌè¨ÌÅ¨ ÎèôÏïà Í∞úÏÑ† ÏóÜÏúºÎ©¥ Ï§ëÎã®
        
        # Í∏∞ÌÉÄ
        verbose=True,
        seed=42,
    )
    
    # ÏµúÏ¢Ö Î™®Îç∏ Ï†ÄÏû•
    model.save('weights/thermal_baseline_best.pt')
    
    return results

if __name__ == '__main__':
    # GPU ÌôïÏù∏
    print(f"CUDA Available: {torch.cuda.is_available()}")
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    
    # ÌõàÎ†® ÏãúÏûë
    results = train_thermal_baseline()
    
    # Í≤∞Í≥º Ï∂úÎ†•
    print("\n=== Phase 1 Training Complete ===")
    print(f"Best mAP50: {results.results_dict['metrics/mAP50(B)']:.4f}")
    print(f"Best mAP50-95: {results.results_dict['metrics/mAP50-95(B)']:.4f}")
```

## 1-2. ÌõàÎ†® Ïã§Ìñâ

```bash
# Phase 1 ÏãúÏûë
python scripts/phase1_thermal.py

# ÏòàÏÉÅ ÏÜåÏöî ÏãúÍ∞Ñ (RTX 4090 Í∏∞Ï§Ä)
# - 100 epochs, 700 images, batch 16
# - ÏïΩ 2-4ÏãúÍ∞Ñ
```

## 1-3. ÌõàÎ†® Î™®ÎãàÌÑ∞ÎßÅ

```bash
# TensorBoard Ïã§Ìñâ
tensorboard --logdir runs/phase1

# Î∏åÎùºÏö∞Ï†ÄÏóêÏÑú http://localhost:6006 Ï†ëÏÜç
```

**Ï£ºÏöî Î™®ÎãàÌÑ∞ÎßÅ ÏßÄÌëú**:
- `mAP50`: 50% IoUÏóêÏÑúÏùò mAP (Ï£ºÏöî ÏßÄÌëú)
- `mAP50-95`: IoU 50~95% ÌèâÍ∑† mAP (ÏóÑÍ≤©Ìïú ÌèâÍ∞Ä)
- `Precision`: Ï†ïÎ∞ÄÎèÑ (ÌÉêÏßÄÌïú Í≤É Ï§ë Ïã§Ï†ú Person ÎπÑÏú®)
- `Recall`: Ïû¨ÌòÑÏú® (Ïã§Ï†ú Person Ï§ë ÌÉêÏßÄÌïú ÎπÑÏú®)
- `Box Loss`: BBox ÏúÑÏπò Ï†ïÌôïÎèÑ
- `Class Loss`: ÌÅ¥ÎûòÏä§ Î∂ÑÎ•ò ÏÜêÏã§

## 1-4. Phase 1 ÏÑ±Í≥µ Í∏∞Ï§Ä

```
Î™©Ìëú ÏÑ±Îä•:
- mAP50 > 0.85 (85% Ïù¥ÏÉÅ)
- mAP50-95 > 0.60 (60% Ïù¥ÏÉÅ)
- Precision > 0.80
- Recall > 0.75

ÏµúÏÜå ÏÑ±Îä• (Phase 3 ÏßÑÌñâ Í∏∞Ï§Ä):
- mAP50 > 0.70
- mAP50-95 > 0.45
```

---

# Phase 2: Attention Fusion 3-way Î™®Îç∏ Íµ¨ÌòÑ

## 2-1. Attention Fusion Î™®Îìà Íµ¨ÌòÑ

**models/attention_fusion.py**:
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class CrossModalAttention(nn.Module):
    """Cross-Modal Attention for multi-modal fusion"""
    
    def __init__(self, dim, num_heads=8, dropout=0.1):
        """
        Args:
            dim: Feature dimension
            num_heads: Number of attention heads
            dropout: Dropout rate
        """
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        
        assert dim % num_heads == 0, "dim must be divisible by num_heads"
        
        # Multi-head attention
        self.qkv = nn.Linear(dim, dim * 3)
        self.attn_drop = nn.Dropout(dropout)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(dropout)
        
        # Normalization
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)
        
        # Feed-forward network
        self.mlp = nn.Sequential(
            nn.Linear(dim, dim * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(dim * 4, dim),
            nn.Dropout(dropout)
        )
    
    def forward(self, thermal_feat, nvg_feat, rgb_feat):
        """
        Args:
            thermal_feat: [B, C, H, W]
            nvg_feat: [B, C, H, W]
            rgb_feat: [B, C, H, W]
        
        Returns:
            fused_feat: [B, C, H, W]
        """
        B, C, H, W = thermal_feat.shape
        
        # Stack modalities: [3, B, C, H, W]
        x = torch.stack([thermal_feat, nvg_feat, rgb_feat], dim=0)
        
        # Reshape to [3, B, HW, C] for attention
        x = x.flatten(3).transpose(2, 3)  # [3, B, HW, C]
        num_modalities = x.shape[0]
        
        # Flatten modalities: [3*B, HW, C]
        x_flat = x.reshape(-1, H*W, C)
        
        # Self-attention
        shortcut = x_flat
        x_flat = self.norm1(x_flat)
        
        # Compute Q, K, V
        qkv = self.qkv(x_flat).reshape(-1, H*W, 3, self.num_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, 3*B, num_heads, HW, head_dim]
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        # Attention
        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn = F.softmax(attn, dim=-1)
        attn = self.attn_drop(attn)
        
        # Apply attention to values
        x_flat = (attn @ v).transpose(1, 2).reshape(-1, H*W, C)
        x_flat = self.proj(x_flat)
        x_flat = self.proj_drop(x_flat)
        x_flat = shortcut + x_flat
        
        # Feed-forward
        x_flat = x_flat + self.mlp(self.norm2(x_flat))
        
        # Reshape back: [3*B, HW, C] -> [3, B, C, H, W]
        x = x_flat.reshape(num_modalities, B, H, W, C).permute(0, 1, 4, 2, 3)
        
        # Average across modalities (or learnable weighted sum)
        fused_feat = x.mean(dim=0)  # [B, C, H, W]
        
        return fused_feat


class LearnableModalityWeights(nn.Module):
    """Learnable weights for modality fusion (optional enhancement)"""
    
    def __init__(self, num_modalities=3):
        super().__init__()
        self.weights = nn.Parameter(torch.ones(num_modalities) / num_modalities)
    
    def forward(self, *modality_feats):
        """
        Args:
            modality_feats: List of [B, C, H, W] tensors
        
        Returns:
            weighted_feat: [B, C, H, W]
        """
        # Softmax normalization
        weights = F.softmax(self.weights, dim=0)
        
        # Weighted sum
        weighted = sum(w * feat for w, feat in zip(weights, modality_feats))
        
        return weighted
```

## 2-2. Multi-modal YOLO Î™®Îç∏ Íµ¨ÌòÑ

**models/multimodal_yolo.py**:
```python
import torch
import torch.nn as nn
from ultralytics import YOLO
from .attention_fusion import CrossModalAttention

class MultiModalYOLO(nn.Module):
    """
    3-way Multi-modal YOLO with Attention Fusion
    ÏûÖÎ†•: Thermal, NVG, RGB Enhanced
    Ï∂úÎ†•: YOLO detection
    """
    
    def __init__(self, 
                 backbone_weights='yolo11x.pt',
                 fusion_dim=512,
                 num_heads=8,
                 dropout=0.1):
        """
        Args:
            backbone_weights: YOLO ÏÇ¨Ï†ÑÌïôÏäµ Í∞ÄÏ§ëÏπò Í≤ΩÎ°ú
            fusion_dim: Fusion layerÏùò feature dimension
            num_heads: Attention heads Ïàò
            dropout: Dropout rate
        """
        super().__init__()
        
        # Load YOLO base model
        base_model = YOLO(backbone_weights).model
        
        # Create 3 separate backbones for each modality
        self.thermal_backbone = self._create_backbone(base_model)
        self.nvg_backbone = self._create_backbone(base_model)
        self.rgb_backbone = self._create_backbone(base_model)
        
        # Attention fusion module
        self.fusion = CrossModalAttention(
            dim=fusion_dim,
            num_heads=num_heads,
            dropout=dropout
        )
        
        # Detection head (shared across modalities)
        self.detection_head = base_model.model[-1]  # YOLO head
        
        # Feature dimension adapter (if needed)
        self.feat_adapter = nn.Conv2d(
            in_channels=1024,  # YOLO11x backbone output
            out_channels=fusion_dim,
            kernel_size=1
        )
    
    def _create_backbone(self, base_model):
        """Create a copy of YOLO backbone"""
        import copy
        backbone = copy.deepcopy(base_model.model[:-1])  # Exclude head
        return backbone
    
    def forward(self, thermal, nvg, rgb_enhanced):
        """
        Args:
            thermal: [B, 3, H, W] or [B, 1, H, W]
            nvg: [B, 3, H, W] or [B, 1, H, W]
            rgb_enhanced: [B, 3, H, W]
        
        Returns:
            detections: YOLO output format
        """
        # Extract features from each modality
        feat_thermal = self.thermal_backbone(thermal)
        feat_nvg = self.nvg_backbone(nvg)
        feat_rgb = self.rgb_backbone(rgb_enhanced)
        
        # Get the last feature map (P5 level for YOLO)
        feat_thermal = feat_thermal[-1]  # [B, 1024, H/32, W/32]
        feat_nvg = feat_nvg[-1]
        feat_rgb = feat_rgb[-1]
        
        # Adapt feature dimensions
        feat_thermal = self.feat_adapter(feat_thermal)  # [B, 512, H/32, W/32]
        feat_nvg = self.feat_adapter(feat_nvg)
        feat_rgb = self.feat_adapter(feat_rgb)
        
        # Fusion via Cross-Modal Attention
        fused_feat = self.fusion(feat_thermal, feat_nvg, feat_rgb)
        
        # Detection head
        detections = self.detection_head(fused_feat)
        
        return detections
    
    def load_thermal_weights(self, thermal_model_path):
        """Load pre-trained thermal model weights"""
        thermal_model = YOLO(thermal_model_path).model
        self.thermal_backbone.load_state_dict(
            thermal_model.model[:-1].state_dict()
        )
        print(f"Loaded thermal weights from {thermal_model_path}")
    
    def load_all_backbones_from_thermal(self, thermal_model_path):
        """Initialize all 3 backbones with thermal weights (transfer learning)"""
        thermal_state = YOLO(thermal_model_path).model.model[:-1].state_dict()
        
        self.thermal_backbone.load_state_dict(thermal_state)
        self.nvg_backbone.load_state_dict(thermal_state)
        self.rgb_backbone.load_state_dict(thermal_state)
        
        print(f"Initialized all backbones from {thermal_model_path}")
```

## 2-3. Custom Data Loader Íµ¨ÌòÑ

**models/data_loader.py**:
```python
import torch
from torch.utils.data import Dataset, DataLoader
import cv2
import numpy as np
from pathlib import Path
import albumentations as A
from albumentations.pytorch import ToTensorV2

class MultiModalDataset(Dataset):
    """Multi-modal dataset for thermal + nvg + visual_enhanced"""
    
    def __init__(self, 
                 data_root,
                 split='train',
                 img_size=640,
                 augment=True):
        """
        Args:
            data_root: Îç∞Ïù¥ÌÑ∞ Î£®Ìä∏ Í≤ΩÎ°ú
            split: 'train', 'val', 'test'
            img_size: ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞
            augment: Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï Ïó¨Î∂Ä
        """
        self.data_root = Path(data_root)
        self.split = split
        self.img_size = img_size
        self.augment = augment
        
        # Paths
        self.thermal_dir = self.data_root / split / 'thermal'
        self.nvg_dir = self.data_root / split / 'nvg'
        self.visual_dir = self.data_root / split / 'visual_enhanced'
        self.label_dir = self.data_root / split / 'labels'
        
        # Get image list
        self.image_files = sorted(list(self.thermal_dir.glob('*.jpg')))
        
        # Augmentation
        self.transform = self._get_transforms()
    
    def _get_transforms(self):
        """Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï ÌååÏù¥ÌîÑÎùºÏù∏"""
        if self.augment and self.split == 'train':
            return A.Compose([
                # Í≥µÍ∞Ñ Î≥ÄÌôò (3Í∞ú Î™®Îã¨Î¶¨Ìã∞ ÎèôÏùº Ï†ÅÏö©)
                A.HorizontalFlip(p=0.5),
                A.ShiftScaleRotate(
                    shift_limit=0.1,
                    scale_limit=0.2,
                    rotate_limit=10,
                    p=0.5
                ),
                A.RandomCrop(self.img_size, self.img_size, p=1.0),
                
                # Î∞ùÍ∏∞/ÎåÄÎπÑ (Î™®Îã¨Î¶¨Ìã∞Î≥Ñ ÎèÖÎ¶Ω Ï†ÅÏö©)
                A.RandomBrightnessContrast(p=0.5),
                A.RandomGamma(p=0.3),
                
                # Normalization
                A.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225]),
                ToTensorV2()
            ], additional_targets={'nvg': 'image', 'rgb': 'image'},
               bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))
        else:
            return A.Compose([
                A.Resize(self.img_size, self.img_size),
                A.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225]),
                ToTensorV2()
            ], additional_targets={'nvg': 'image', 'rgb': 'image'},
               bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        # Load images
        img_name = self.image_files[idx].stem
        
        thermal = cv2.imread(str(self.thermal_dir / f'{img_name}.jpg'))
        nvg = cv2.imread(str(self.nvg_dir / f'{img_name}.jpg'))
        rgb = cv2.imread(str(self.rgb_dir / f'{img_name}.jpg'))
        
        thermal = cv2.cvtColor(thermal, cv2.COLOR_BGR2RGB)
        nvg = cv2.cvtColor(nvg, cv2.COLOR_BGR2RGB)
        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)
        
        # Load labels (YOLO format)
        label_file = self.label_dir / f'{img_name}.txt'
        if label_file.exists():
            labels = np.loadtxt(label_file).reshape(-1, 5)
            bboxes = labels[:, 1:5]  # x_center, y_center, w, h
            class_labels = labels[:, 0].astype(int).tolist()
        else:
            bboxes = np.array([])
            class_labels = []
        
        # Apply transforms
        transformed = self.transform(
            image=thermal,
            nvg=nvg,
            rgb=rgb,
            bboxes=bboxes,
            class_labels=class_labels
        )
        
        thermal_tensor = transformed['image']
        nvg_tensor = transformed['nvg']
        rgb_tensor = transformed['rgb']
        bboxes_transformed = np.array(transformed['bboxes'])
        class_labels_transformed = transformed['class_labels']
        
        # Convert to YOLO label format
        if len(bboxes_transformed) > 0:
            labels_tensor = torch.zeros((len(bboxes_transformed), 6))
            labels_tensor[:, 0] = 0  # Batch index (filled by collate_fn)
            labels_tensor[:, 1] = torch.tensor(class_labels_transformed)
            labels_tensor[:, 2:] = torch.tensor(bboxes_transformed)
        else:
            labels_tensor = torch.zeros((0, 6))
        
        return {
            'thermal': thermal_tensor,
            'nvg': nvg_tensor,
            'rgb': rgb_tensor,
            'labels': labels_tensor,
            'img_name': img_name
        }


def collate_fn(batch):
    """Custom collate function for multi-modal batches"""
    thermal_batch = torch.stack([item['thermal'] for item in batch])
    nvg_batch = torch.stack([item['nvg'] for item in batch])
    rgb_batch = torch.stack([item['rgb'] for item in batch])
    
    # Labels with batch index
    labels = []
    for i, item in enumerate(batch):
        l = item['labels']
        l[:, 0] = i  # Set batch index
        labels.append(l)
    labels = torch.cat(labels, 0)
    
    return {
        'thermal': thermal_batch,
        'nvg': nvg_batch,
        'rgb': rgb_batch,
        'labels': labels
    }
```

---

# Phase 3: Multi-modal Î™®Îç∏ Fine-tuning ÌõàÎ†®

## 3-1. ÌõàÎ†® Ïä§ÌÅ¨Î¶ΩÌä∏

**scripts/phase3_multimodal.py**:
```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path
import yaml
from tqdm import tqdm
import numpy as np

import sys
sys.path.append('..')
from models.multimodal_yolo import MultiModalYOLO
from models.data_loader import MultiModalDataset, collate_fn

class MultiModalTrainer:
    def __init__(self, config_path='configs/hyperparams.yaml'):
        # Load config
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        # Device
        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {self.device}")
        
        # Model
        self.model = MultiModalYOLO(
            backbone_weights=self.config['model']['backbone_weights'],
            fusion_dim=self.config['model']['fusion_dim'],
            num_heads=self.config['model']['num_heads'],
            dropout=self.config['model']['dropout']
        ).to(self.device)
        
        # Load Phase 1 thermal weights
        thermal_weights = self.config['pretrain']['thermal_model_path']
        self.model.load_all_backbones_from_thermal(thermal_weights)
        
        # Data loaders
        self.train_loader = self._create_dataloader('train')
        self.val_loader = self._create_dataloader('val')
        
        # Optimizer
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config['training']['lr0'],
            weight_decay=self.config['training']['weight_decay']
        )
        
        # Scheduler
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=self.config['training']['epochs'],
            eta_min=self.config['training']['lr0'] * self.config['training']['lrf']
        )
        
        # Loss function (YOLO loss from Ultralytics)
        from ultralytics.utils.loss import v8DetectionLoss
        self.criterion = v8DetectionLoss(self.model.detection_head)
        
        # AMP Scaler
        self.scaler = torch.cuda.amp.GradScaler(enabled=self.config['training']['amp'])
        
        # Metrics
        self.best_map = 0.0
        self.train_losses = []
        self.val_maps = []
    
    def _create_dataloader(self, split):
        dataset = MultiModalDataset(
            data_root=self.config['data']['root'],
            split=split,
            img_size=self.config['training']['imgsz'],
            augment=(split == 'train')
        )
        
        loader = DataLoader(
            dataset,
            batch_size=self.config['training']['batch'],
            shuffle=(split == 'train'),
            num_workers=self.config['training']['workers'],
            pin_memory=True,
            collate_fn=collate_fn
        )
        
        return loader
    
    def train_one_epoch(self, epoch):
        self.model.train()
        epoch_loss = 0.0
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}')
        for batch_idx, batch in enumerate(pbar):
            # Move to device
            thermal = batch['thermal'].to(self.device)
            nvg = batch['nvg'].to(self.device)
            rgb = batch['rgb'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            # Forward with AMP
            with torch.cuda.amp.autocast(enabled=self.config['training']['amp']):
                predictions = self.model(thermal, nvg, rgb)
                loss = self.criterion(predictions, labels)
            
            # Backward
            self.optimizer.zero_grad()
            self.scaler.scale(loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()
            
            # Log
            epoch_loss += loss.item()
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        avg_loss = epoch_loss / len(self.train_loader)
        self.train_losses.append(avg_loss)
        
        return avg_loss
    
    @torch.no_grad()
    def validate(self, epoch):
        self.model.eval()
        
        # TODO: Implement mAP calculation
        # For now, placeholder
        val_map = 0.0
        
        self.val_maps.append(val_map)
        
        # Save best model
        if val_map > self.best_map:
            self.best_map = val_map
            save_path = Path('weights/multimodal_best.pt')
            save_path.parent.mkdir(exist_ok=True)
            torch.save({
                'epoch': epoch,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'mAP': val_map,
            }, save_path)
            print(f"Best model saved: mAP={val_map:.4f}")
        
        return val_map
    
    def train(self):
        epochs = self.config['training']['epochs']
        
        for epoch in range(epochs):
            # Train
            train_loss = self.train_one_epoch(epoch)
            
            # Validate
            val_map = self.validate(epoch)
            
            # Scheduler step
            self.scheduler.step()
            
            # Log
            print(f"Epoch {epoch+1}/{epochs} - "
                  f"Train Loss: {train_loss:.4f}, "
                  f"Val mAP: {val_map:.4f}, "
                  f"LR: {self.optimizer.param_groups[0]['lr']:.6f}")
            
            # Save checkpoint
            if (epoch + 1) % self.config['training']['save_period'] == 0:
                save_path = Path(f'weights/multimodal_epoch{epoch+1}.pt')
                save_path.parent.mkdir(exist_ok=True)
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                }, save_path)


if __name__ == '__main__':
    trainer = MultiModalTrainer(config_path='configs/hyperparams.yaml')
    trainer.train()
```

## 3-2. ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï ÌååÏùº

**configs/hyperparams.yaml**:
```yaml
# Model configuration
model:
  backbone_weights: 'yolo11x.pt'
  fusion_dim: 512
  num_heads: 8
  dropout: 0.1

# Data configuration
data:
  root: 'data/nighttime'
  nc: 1
  names: ['person']

# Pre-training
pretrain:
  thermal_model_path: 'weights/thermal_baseline_best.pt'

# Training configuration
training:
  # Basic
  epochs: 100
  batch: 12              # RTX 4090 24GB Í∏∞Ï§Ä (Ï°∞Ï†ï Í∞ÄÎä•)
  imgsz: 640             # ÎòêÎäî 1280 (Í≥†Ìï¥ÏÉÅÎèÑ)
  workers: 8
  device: 0
  seed: 42
  
  # Optimizer
  optimizer: 'AdamW'
  lr0: 0.0001            # Fine-tuningÏù¥ÎØÄÎ°ú ÎÇÆÏùÄ ÌïôÏäµÎ•†
  lrf: 0.01              # ÏµúÏ¢Ö lr = lr0 * lrf
  momentum: 0.937
  weight_decay: 0.0005
  
  # Scheduler
  cos_lr: true
  
  # Performance
  amp: true              # Automatic Mixed Precision
  
  # Saving
  save_period: 10
  
  # Early stopping
  patience: 50

# Validation
validation:
  conf_thres: 0.25       # Confidence threshold
  iou_thres: 0.45        # NMS IoU threshold
```

## 3-3. ÌõàÎ†® Ïã§Ìñâ

```bash
# Phase 3 ÏãúÏûë
python scripts/phase3_multimodal.py

# ÏòàÏÉÅ ÏÜåÏöî ÏãúÍ∞Ñ (RTX 4090 Í∏∞Ï§Ä)
# - 100 epochs, 700 images, batch 12
# - ÏïΩ 4-6ÏãúÍ∞Ñ (3Í∞ú Î∞±Î≥∏Ïù¥ÎØÄÎ°ú Phase 1Î≥¥Îã§ Ïò§Îûò Í±∏Î¶º)
```

---

# Phase 4: ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏµúÏ†ÅÌôî

## 4-1. Ïã§ÌóòÌï† ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞

### Ïö∞ÏÑ†ÏàúÏúÑ ÎÜíÏùå
```yaml
# 1. Learning Rate
lr0: [0.0001, 0.0005, 0.001]

# 2. Batch Size
batch: [8, 12, 16]  # Î©îÎ™®Î¶¨ ÌóàÏö© Î≤îÏúÑ ÎÇ¥

# 3. Image Size
imgsz: [640, 1280]  # Í≥†Ìï¥ÏÉÅÎèÑ Ïã§Ìóò

# 4. Fusion Dimension
fusion_dim: [256, 512, 1024]

# 5. Attention Heads
num_heads: [4, 8, 16]
```

### Ïö∞ÏÑ†ÏàúÏúÑ Ï§ëÍ∞Ñ
```yaml
# 6. Dropout
dropout: [0.0, 0.1, 0.2]

# 7. Weight Decay
weight_decay: [0.0001, 0.0005, 0.001]

# 8. Data Augmentation strength
hsv_v: [0.2, 0.4, 0.6]
degrees: [5, 10, 15]
```

## 4-2. Grid Search Ïä§ÌÅ¨Î¶ΩÌä∏

**scripts/hyperparameter_search.py**:
```python
import itertools
import yaml

# Ïã§ÌóòÌï† Ï°∞Ìï©
lr_values = [0.0001, 0.0005]
batch_values = [12, 16]
imgsz_values = [640, 1280]

experiments = list(itertools.product(lr_values, batch_values, imgsz_values))

for i, (lr, batch, imgsz) in enumerate(experiments):
    print(f"\n=== Experiment {i+1}/{len(experiments)} ===")
    print(f"lr={lr}, batch={batch}, imgsz={imgsz}")
    
    # Update config
    config = {
        'model': {...},
        'training': {
            'lr0': lr,
            'batch': batch,
            'imgsz': imgsz,
            ...
        }
    }
    
    # Save temporary config
    with open(f'configs/exp_{i}.yaml', 'w') as f:
        yaml.dump(config, f)
    
    # Run training
    # trainer = MultiModalTrainer(config_path=f'configs/exp_{i}.yaml')
    # results = trainer.train()
    
    # Log results
    # ...
```

---

# Phase 5: ÏµúÏ¢Ö ÌèâÍ∞Ä Î∞è Î∞∞Ìè¨

## 5-1. ÌèâÍ∞Ä Ïä§ÌÅ¨Î¶ΩÌä∏

**scripts/evaluate.py**:
```python
import torch
from pathlib import Path
import cv2
import numpy as np
from models.multimodal_yolo import MultiModalYOLO

def evaluate_model(model_path, test_data_path):
    """ÏµúÏ¢Ö Î™®Îç∏ ÌèâÍ∞Ä"""
    
    # Load model
    model = MultiModalYOLO()
    checkpoint = torch.load(model_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    model.cuda()
    
    # Metrics
    total_tp = 0
    total_fp = 0
    total_fn = 0
    
    # Test images
    test_dir = Path(test_data_path)
    
    for img_path in test_dir.glob('*.jpg'):
        # Load 3 modalities
        thermal = cv2.imread(str(img_path))  # Load thermal
        nvg = cv2.imread(...)  # Load nvg
        rgb = cv2.imread(...)  # Load rgb_enhanced
        
        # Preprocess
        # ...
        
        # Inference
        with torch.no_grad():
            predictions = model(thermal, nvg, rgb)
        
        # Compute metrics
        # ...
    
    # Calculate mAP
    precision = total_tp / (total_tp + total_fp)
    recall = total_tp / (total_tp + total_fn)
    f1 = 2 * (precision * recall) / (precision + recall)
    
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")
    
    return precision, recall, f1

if __name__ == '__main__':
    evaluate_model(
        model_path='weights/multimodal_best.pt',
        test_data_path='data/nighttime/test'
    )
```

## 5-2. Ï∂îÎ°† Î∞è ÏãúÍ∞ÅÌôî

**scripts/inference.py**:
```python
import torch
import cv2
import numpy as np
from models.multimodal_yolo import MultiModalYOLO

def inference_single_image(model, thermal_path, nvg_path, rgb_path):
    """Îã®Ïùº Ïù¥ÎØ∏ÏßÄ Ï∂îÎ°†"""
    
    # Load images
    thermal = cv2.imread(thermal_path)
    nvg = cv2.imread(nvg_path)
    rgb = cv2.imread(rgb_path)
    
    # Preprocess
    # ... (normalize, resize, to_tensor)
    
    # Inference
    model.eval()
    with torch.no_grad():
        predictions = model(thermal, nvg, rgb)
    
    # Post-process (NMS, threshold)
    # ...
    
    # Draw bounding boxes
    for bbox in predictions:
        x1, y1, x2, y2, conf, cls = bbox
        cv2.rectangle(thermal, (x1, y1), (x2, y2), (255, 128, 0), 2)
        cv2.putText(thermal, f'Person {conf:.2f}', 
                   (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 
                   0.5, (255, 128, 0), 2)
    
    # Save result
    cv2.imwrite('output/detection_result.jpg', thermal)
    
    return predictions

if __name__ == '__main__':
    model = MultiModalYOLO()
    model.load_state_dict(torch.load('weights/multimodal_best.pt')['model_state_dict'])
    model.cuda()
    
    predictions = inference_single_image(
        model,
        'test_thermal.jpg',
        'test_nvg.jpg',
        'test_rgb_enhanced.jpg'
    )
```

---

# ÏµúÏ¢Ö Ï≤¥ÌÅ¨Î¶¨Ïä§Ìä∏

## Phase 0: ÌôòÍ≤Ω ÏÑ§Ï†ï
- [ ] Conda ÌôòÍ≤Ω ÏÉùÏÑ±
- [ ] PyTorch + CUDA ÏÑ§Ïπò
- [ ] Ultralytics YOLO ÏÑ§Ïπò
- [ ] ÌîÑÎ°úÏ†ùÌä∏ ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞ ÏÉùÏÑ±
- [ ] Îç∞Ïù¥ÌÑ∞ÏÖã Ï§ÄÎπÑ (YOLO ÌòïÏãù)
- [ ] YAML ÌååÏùº ÏûëÏÑ±

## Phase 1: Baseline
- [ ] thermal.yaml ÏÑ§Ï†ï
- [ ] Phase 1 ÌõàÎ†® Ïä§ÌÅ¨Î¶ΩÌä∏ Ïã§Ìñâ
- [ ] TensorBoard Î™®ÎãàÌÑ∞ÎßÅ
- [ ] mAP > 0.70 ÌôïÏù∏
- [ ] Best weights Ï†ÄÏû•

## Phase 2: Î™®Îç∏ Íµ¨ÌòÑ
- [ ] attention_fusion.py ÏûëÏÑ±
- [ ] multimodal_yolo.py ÏûëÏÑ±
- [ ] data_loader.py ÏûëÏÑ±
- [ ] Î™®Îç∏ Íµ¨Ï°∞ ÌÖåÏä§Ìä∏ (forward pass)

## Phase 3: Fine-tuning
- [ ] hyperparams.yaml ÏÑ§Ï†ï
- [ ] Phase 1 Í∞ÄÏ§ëÏπò Î°úÎìú ÌôïÏù∏
- [ ] Phase 3 ÌõàÎ†® ÏãúÏûë
- [ ] Loss Í∞êÏÜå Î™®ÎãàÌÑ∞ÎßÅ
- [ ] Best model Ï†ÄÏû•

## Phase 4: ÏµúÏ†ÅÌôî
- [ ] ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Ïã§Ìóò
- [ ] Ablation study (Î™®Îã¨Î¶¨Ìã∞ Ï†úÍ±∞ Ïã§Ìóò)
- [ ] ÏµúÏ†Å Ï°∞Ìï© ÏÑ†ÌÉù

## Phase 5: ÌèâÍ∞Ä
- [ ] Test set ÌèâÍ∞Ä
- [ ] mAP, Precision, Recall Ï∏°Ï†ï
- [ ] ÏãúÍ∞ÅÌôî Í≤∞Í≥º ÌôïÏù∏
- [ ] ÏµúÏ¢Ö Î™®Îç∏ Ï†ÄÏû•

---

# ÏòàÏÉÅ ÏÑ±Îä• Î™©Ìëú

## Phase 1 (Baseline)
```
Ïó¥ÏÉÅ Îã®Ïùº Î™®Îã¨:
- mAP50: 0.75 ~ 0.85
- mAP50-95: 0.50 ~ 0.65
```

## Phase 3 (Multi-modal)
```
3-way Attention Fusion:
- mAP50: 0.85 ~ 0.92 (Baseline ÎåÄÎπÑ +10%)
- mAP50-95: 0.65 ~ 0.75 (Baseline ÎåÄÎπÑ +15%)
- Precision: 0.85+
- Recall: 0.80+
```

## ÏÑ±Îä• Ìñ•ÏÉÅ Í∑ºÍ±∞
1. **Ïó¥ÏÉÅ**: Ïò®ÎèÑ Í∏∞Î∞ò ÏÇ¨Îûå ÌÉêÏßÄ (Í∏∞Î≥∏)
2. **ÏïºÍ∞ÑÌà¨Ïãú**: Í∑ºÏ†ÅÏô∏ÏÑ† ÎîîÌÖåÏùº Ï∂îÍ∞Ä
3. **Enhanced Í∞ÄÏãúÍ¥ë**: RGB ÏÉâÏÉÅ Ï†ïÎ≥¥ Î≥¥ÏôÑ
4. **Attention Fusion**: ÏÉÅÌô©Î≥Ñ ÏµúÏ†Å Î™®Îã¨Î¶¨Ìã∞ ÏûêÎèô ÏÑ†ÌÉù

---

# Î¨∏Ï†ú Ìï¥Í≤∞ Í∞ÄÏù¥Îìú

## 1. CUDA Out of Memory
```python
# Ìï¥Í≤∞Ï±Ö 1: Batch size Ï§ÑÏù¥Í∏∞
batch: 12 ‚Üí 8

# Ìï¥Í≤∞Ï±Ö 2: Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞ Ï§ÑÏù¥Í∏∞
imgsz: 640 (1280 ÎåÄÏã†)

# Ìï¥Í≤∞Ï±Ö 3: Gradient Accumulation
accumulate: 2  # Effective batch = batch * accumulate
```

## 2. LossÍ∞Ä Í∞êÏÜåÌïòÏßÄ ÏïäÏùå
```python
# Ï≤¥ÌÅ¨ ÏÇ¨Ìï≠:
1. Learning rate ÎÑàÎ¨¥ ÎÜíÏùå ‚Üí 0.0001Î°ú ÎÇÆÏ∂îÍ∏∞
2. Îç∞Ïù¥ÌÑ∞ Î°úÎçî ÌôïÏù∏ (ÎùºÎ≤®Ïù¥ Ïò¨Î∞îÎ•∏ÏßÄ)
3. Phase 1 Í∞ÄÏ§ëÏπòÍ∞Ä Ï†úÎåÄÎ°ú Î°úÎìúÎêòÏóàÎäîÏßÄ ÌôïÏù∏
4. Gradient clipping Ï∂îÍ∞Ä:
   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)
```

## 3. Overfitting
```python
# Ìï¥Í≤∞Ï±Ö:
1. Dropout Ï¶ùÍ∞Ä: 0.1 ‚Üí 0.2
2. Weight decay Ï¶ùÍ∞Ä: 0.0005 ‚Üí 0.001
3. Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï Í∞ïÌôî
4. Early stopping patience Ï§ÑÏù¥Í∏∞
```

## 4. Ìïú Î™®Îã¨Î¶¨Ìã∞Îßå ÌïôÏäµÎê®
```python
# Attention weights Î™®ÎãàÌÑ∞ÎßÅ
# LearnableModalityWeights Ï∂îÍ∞ÄÌïòÏó¨ Í∞Å Î™®Îã¨Î¶¨Ìã∞ Í∏∞Ïó¨ÎèÑ ÌôïÏù∏
# ÌïÑÏöîÏãú modality-specific loss balancing
```

---

Ïù¥Ï†ú Ïù¥ Î¨∏ÏÑúÎåÄÎ°ú Îã®Í≥ÑÎ≥ÑÎ°ú ÏßÑÌñâÌïòÏãúÎ©¥ Îê©ÎãàÎã§! üöÄ
Ï∂îÍ∞Ä ÏßàÎ¨∏Ïù¥ÎÇò ÎßâÌûàÎäî Î∂ÄÎ∂Ñ ÏûàÏúºÎ©¥ Ïñ∏Ï†úÎì† Î¨ºÏñ¥Î≥¥ÏÑ∏Ïöî!