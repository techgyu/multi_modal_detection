# Attention-based Multi-modal YOLO í›ˆë ¨ ê³„íš (ìµœì¢…)

**ëª©í‘œ**: ì—´ìƒ + ì•¼ê°„íˆ¬ì‹œ + Enhanced ê°€ì‹œê´‘ 3-way ì…ë ¥ìœ¼ë¡œ ì•¼ê°„ Person íƒì§€  
**ì•„í‚¤í…ì²˜**: Attention-based Late Fusion  
**í•˜ë“œì›¨ì–´**: RTX 4090 24GB, Ryzen 7 7800X, 32GB DDR5

---

## ğŸ“‹ ì „ì²´ í›ˆë ¨ ë¡œë“œë§µ

```
Phase 0: í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ì¤€ë¹„
    â†“
Phase 1: ì—´ìƒ ë‹¨ì¼ ëª¨ë‹¬ Baseline ëª¨ë¸ í›ˆë ¨ (Plan C)
    â†“
Phase 2: Attention Fusion 3-way ëª¨ë¸ êµ¬í˜„
    â†“
Phase 3: ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ë¡œë“œ ë° Fine-tuning
    â†“
Phase 4: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
    â†“
Phase 5: ìµœì¢… í‰ê°€ ë° ë°°í¬
```

---

# Phase 0: í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ì¤€ë¹„

## 0-1. ê°œë°œ í™˜ê²½ êµ¬ì¶•

### Python í™˜ê²½
```bash
# Conda ê°€ìƒí™˜ê²½ ìƒì„± (Python 3.10 ê¶Œì¥)
conda create -n multimodal_detection python=3.10
conda activate multimodal_detection
```

### í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
# PyTorch (CUDA 12.1 ê¸°ì¤€)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Ultralytics YOLO
pip install ultralytics

# ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install albumentations opencv-python pillow matplotlib tensorboard
pip install pandas numpy scikit-learn tqdm
pip install timm einops  # Attention ëª¨ë“ˆìš©
```

### í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°
```
multi_modal_detection/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ daytime/          # ì£¼ê°„ ë°ì´í„°
â”‚   â”‚   â”œâ”€â”€ thermal/      # ì—´ìƒ
â”‚   â”‚   â”œâ”€â”€ visual/       # ê°€ì‹œê´‘
â”‚   â”‚   â””â”€â”€ labels/       # YOLO í˜•ì‹ ë¼ë²¨
â”‚   â”œâ”€â”€ nighttime/        # ì•¼ê°„ ë°ì´í„°
â”‚   â”‚   â”œâ”€â”€ thermal/      # ì—´ìƒ
â”‚   â”‚   â”œâ”€â”€ nvg/          # ì•¼ê°„íˆ¬ì‹œ
â”‚   â”‚   â”œâ”€â”€ visual/       # ì›ë³¸ ê°€ì‹œê´‘
â”‚   â”‚   â”œâ”€â”€ visual_enhanced/ # Enhanced ê°€ì‹œê´‘
â”‚   â”‚   â””â”€â”€ labels/       # YOLO í˜•ì‹ ë¼ë²¨
â”‚   â””â”€â”€ yaml/
â”‚       â”œâ”€â”€ thermal.yaml      # Phase 1ìš©
â”‚       â””â”€â”€ multimodal.yaml   # Phase 3ìš©
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ attention_fusion.py   # Attention ëª¨ë“ˆ
â”‚   â”œâ”€â”€ multimodal_yolo.py    # ë©”ì¸ ëª¨ë¸
â”‚   â””â”€â”€ data_loader.py        # ë°ì´í„° ë¡œë”
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ phase1_thermal.py     # Phase 1 í›ˆë ¨
â”‚   â”œâ”€â”€ phase3_multimodal.py  # Phase 3 í›ˆë ¨
â”‚   â””â”€â”€ evaluate.py           # í‰ê°€
â”œâ”€â”€ weights/                  # í•™ìŠµëœ ê°€ì¤‘ì¹˜ ì €ì¥
â”œâ”€â”€ runs/                     # TensorBoard ë¡œê·¸
â””â”€â”€ configs/
    â””â”€â”€ hyperparams.yaml      # í•˜ì´í¼íŒŒë¼ë¯¸í„°
```

## 0-2. ë°ì´í„°ì…‹ ì¤€ë¹„

### YOLO í˜•ì‹ ë¼ë²¨ êµ¬ì¡°
```
# labels/image001.txt
# class_id x_center y_center width height (ì •ê·œí™” 0~1)
0 0.512 0.348 0.124 0.256
0 0.723 0.512 0.098 0.187
```

### ë°ì´í„°ì…‹ YAML íŒŒì¼ ì‘ì„±

**thermal.yaml** (Phase 1ìš©):
```yaml
# data/yaml/thermal.yaml
path: ../data/daytime  # ë°ì´í„°ì…‹ ë£¨íŠ¸
train: thermal/train   # í•™ìŠµ ì´ë¯¸ì§€ í´ë”
val: thermal/val       # ê²€ì¦ ì´ë¯¸ì§€ í´ë”
test: thermal/test     # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ í´ë”

# Classes
nc: 1  # í´ë˜ìŠ¤ ê°œìˆ˜
names: ['person']  # í´ë˜ìŠ¤ ì´ë¦„
```

**multimodal.yaml** (Phase 3ìš©):
```yaml
# data/yaml/multimodal.yaml
path: ../data/nighttime
train: train
val: val
test: test

# Modality paths (custom field)
thermal_path: thermal
nvg_path: nvg
visual_enhanced_path: visual_enhanced

nc: 1
names: ['person']
```

### ë°ì´í„° ë¶„í•  ë¹„ìœ¨
```python
# ì „ì²´ ë°ì´í„° ë¶„í• 
Train: 70% (í•™ìŠµ)
Val:   15% (ê²€ì¦, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹)
Test:  15% (ìµœì¢… í‰ê°€)

# ì˜ˆì‹œ: 1000ì¥ ìˆ˜ì§‘ ì‹œ
Train: 700ì¥
Val:   150ì¥
Test:  150ì¥
```

---

# Phase 1: ì—´ìƒ ë‹¨ì¼ ëª¨ë‹¬ Baseline ëª¨ë¸ í›ˆë ¨

**ëª©ì **: ì—´ìƒë§Œìœ¼ë¡œ Person íƒì§€ ëŠ¥ë ¥ í™•ë³´ (Phase 3ì˜ ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©)

## 1-1. í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸

**scripts/phase1_thermal.py**:
```python
from ultralytics import YOLO
import torch

def train_thermal_baseline():
    """Phase 1: ì—´ìƒ ë‹¨ì¼ ëª¨ë‹¬ ëª¨ë¸ í›ˆë ¨"""
    
    # YOLO11x ëª¨ë¸ ë¡œë“œ (COCO ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜)
    model = YOLO('yolo11x.pt')
    
    # í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    results = model.train(
        # ë°ì´í„°
        data='data/yaml/thermal.yaml',
        
        # ëª¨ë¸ ì„¤ì •
        imgsz=640,              # ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°
        
        # í›ˆë ¨ íŒŒë¼ë¯¸í„°
        epochs=100,             # ì—í¬í¬ ìˆ˜
        batch=16,               # ë°°ì¹˜ ì‚¬ì´ì¦ˆ (4090 24GB ê¸°ì¤€)
        
        # Optimizer
        optimizer='AdamW',      # Adam with weight decay
        lr0=0.001,              # ì´ˆê¸° í•™ìŠµë¥ 
        lrf=0.01,               # ìµœì¢… í•™ìŠµë¥  (lr0 * lrf)
        momentum=0.937,
        weight_decay=0.0005,
        
        # Scheduler
        cos_lr=True,            # Cosine annealing LR scheduler
        
        # Augmentation
        hsv_h=0.015,            # ìƒ‰ì¡° ë³€í™” (ì—´ìƒì€ ë‹¨ìƒ‰ì´ë¯€ë¡œ ì‘ê²Œ)
        hsv_s=0.4,              # ì±„ë„
        hsv_v=0.4,              # ëª…ë„
        degrees=10.0,           # íšŒì „ ê°ë„
        translate=0.1,          # ì´ë™
        scale=0.5,              # ìŠ¤ì¼€ì¼
        shear=0.0,              # ì „ë‹¨ ë³€í˜•
        perspective=0.0001,     # ì›ê·¼ ë³€í˜•
        flipud=0.0,             # ìƒí•˜ ë°˜ì „ (ì‚¬ëŒì€ ë³´í†µ ì•ˆ í•¨)
        fliplr=0.5,             # ì¢Œìš° ë°˜ì „
        mosaic=1.0,             # Mosaic augmentation
        mixup=0.1,              # Mixup augmentation
        
        # ì •ê·œí™”
        dropout=0.0,            # Dropout (YOLOëŠ” ë³´í†µ ì‚¬ìš© ì•ˆ í•¨)
        
        # ì„±ëŠ¥ ìµœì í™”
        amp=True,               # Automatic Mixed Precision (FP16)
        workers=8,              # ë°ì´í„° ë¡œë” ì›Œì»¤ (CPU ì½”ì–´ ìˆ˜)
        device=0,               # GPU 0ë²ˆ ì‚¬ìš©
        
        # ì €ì¥ ë° ë¡œê¹…
        project='runs/phase1',
        name='thermal_baseline',
        exist_ok=True,
        save=True,
        save_period=10,         # 10 ì—í¬í¬ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        
        # Validation
        val=True,
        plots=True,             # í›ˆë ¨ í”Œë¡¯ ìƒì„±
        
        # Early stopping
        patience=50,            # 50 ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨
        
        # ê¸°íƒ€
        verbose=True,
        seed=42,
    )
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    model.save('weights/thermal_baseline_best.pt')
    
    return results

if __name__ == '__main__':
    # GPU í™•ì¸
    print(f"CUDA Available: {torch.cuda.is_available()}")
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    
    # í›ˆë ¨ ì‹œì‘
    results = train_thermal_baseline()
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n=== Phase 1 Training Complete ===")
    print(f"Best mAP50: {results.results_dict['metrics/mAP50(B)']:.4f}")
    print(f"Best mAP50-95: {results.results_dict['metrics/mAP50-95(B)']:.4f}")
```

## 1-2. í›ˆë ¨ ì‹¤í–‰

```bash
# Phase 1 ì‹œì‘
python scripts/phase1_thermal.py

# ì˜ˆìƒ ì†Œìš” ì‹œê°„ (RTX 4090 ê¸°ì¤€)
# - 100 epochs, 700 images, batch 16
# - ì•½ 2-4ì‹œê°„
```

## 1-3. í›ˆë ¨ ëª¨ë‹ˆí„°ë§

```bash
# TensorBoard ì‹¤í–‰
tensorboard --logdir runs/phase1

# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:6006 ì ‘ì†
```

**ì£¼ìš” ëª¨ë‹ˆí„°ë§ ì§€í‘œ**:
- `mAP50`: 50% IoUì—ì„œì˜ mAP (ì£¼ìš” ì§€í‘œ)
- `mAP50-95`: IoU 50~95% í‰ê·  mAP (ì—„ê²©í•œ í‰ê°€)
- `Precision`: ì •ë°€ë„ (íƒì§€í•œ ê²ƒ ì¤‘ ì‹¤ì œ Person ë¹„ìœ¨)
- `Recall`: ì¬í˜„ìœ¨ (ì‹¤ì œ Person ì¤‘ íƒì§€í•œ ë¹„ìœ¨)
- `Box Loss`: BBox ìœ„ì¹˜ ì •í™•ë„
- `Class Loss`: í´ë˜ìŠ¤ ë¶„ë¥˜ ì†ì‹¤

## 1-4. Phase 1 ì„±ê³µ ê¸°ì¤€

```
ëª©í‘œ ì„±ëŠ¥:
- mAP50 > 0.85 (85% ì´ìƒ)
- mAP50-95 > 0.60 (60% ì´ìƒ)
- Precision > 0.80
- Recall > 0.75

ìµœì†Œ ì„±ëŠ¥ (Phase 3 ì§„í–‰ ê¸°ì¤€):
- mAP50 > 0.70
- mAP50-95 > 0.45
```

---

# Phase 2: Attention Fusion 3-way ëª¨ë¸ êµ¬í˜„

## 2-1. Attention Fusion ëª¨ë“ˆ êµ¬í˜„

**models/attention_fusion.py**:
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class CrossModalAttention(nn.Module):
    """Cross-Modal Attention for multi-modal fusion"""
    
    def __init__(self, dim, num_heads=8, dropout=0.1):
        """
        Args:
            dim: Feature dimension
            num_heads: Number of attention heads
            dropout: Dropout rate
        """
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        
        assert dim % num_heads == 0, "dim must be divisible by num_heads"
        
        # Multi-head attention
        self.qkv = nn.Linear(dim, dim * 3)
        self.attn_drop = nn.Dropout(dropout)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(dropout)
        
        # Normalization
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)
        
        # Feed-forward network
        self.mlp = nn.Sequential(
            nn.Linear(dim, dim * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(dim * 4, dim),
            nn.Dropout(dropout)
        )
    
    def forward(self, thermal_feat, nvg_feat, rgb_feat):
        """
        Args:
            thermal_feat: [B, C, H, W]
            nvg_feat: [B, C, H, W]
            rgb_feat: [B, C, H, W]
        
        Returns:
            fused_feat: [B, C, H, W]
        """
        B, C, H, W = thermal_feat.shape
        
        # Stack modalities: [3, B, C, H, W]
        x = torch.stack([thermal_feat, nvg_feat, rgb_feat], dim=0)
        
        # Reshape to [3, B, HW, C] for attention
        x = x.flatten(3).transpose(2, 3)  # [3, B, HW, C]
        num_modalities = x.shape[0]
        
        # Flatten modalities: [3*B, HW, C]
        x_flat = x.reshape(-1, H*W, C)
        
        # Self-attention
        shortcut = x_flat
        x_flat = self.norm1(x_flat)
        
        # Compute Q, K, V
        qkv = self.qkv(x_flat).reshape(-1, H*W, 3, self.num_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, 3*B, num_heads, HW, head_dim]
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        # Attention
        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn = F.softmax(attn, dim=-1)
        attn = self.attn_drop(attn)
        
        # Apply attention to values
        x_flat = (attn @ v).transpose(1, 2).reshape(-1, H*W, C)
        x_flat = self.proj(x_flat)
        x_flat = self.proj_drop(x_flat)
        x_flat = shortcut + x_flat
        
        # Feed-forward
        x_flat = x_flat + self.mlp(self.norm2(x_flat))
        
        # Reshape back: [3*B, HW, C] -> [3, B, C, H, W]
        x = x_flat.reshape(num_modalities, B, H, W, C).permute(0, 1, 4, 2, 3)
        
        # Average across modalities (or learnable weighted sum)
        fused_feat = x.mean(dim=0)  # [B, C, H, W]
        
        return fused_feat


class LearnableModalityWeights(nn.Module):
    """Learnable weights for modality fusion (optional enhancement)"""
    
    def __init__(self, num_modalities=3):
        super().__init__()
        self.weights = nn.Parameter(torch.ones(num_modalities) / num_modalities)
    
    def forward(self, *modality_feats):
        """
        Args:
            modality_feats: List of [B, C, H, W] tensors
        
        Returns:
            weighted_feat: [B, C, H, W]
        """
        # Softmax normalization
        weights = F.softmax(self.weights, dim=0)
        
        # Weighted sum
        weighted = sum(w * feat for w, feat in zip(weights, modality_feats))
        
        return weighted
```

## 2-2. Multi-modal YOLO ëª¨ë¸ êµ¬í˜„

**models/multimodal_yolo.py**:
```python
import torch
import torch.nn as nn
from ultralytics import YOLO
from .attention_fusion import CrossModalAttention

class MultiModalYOLO(nn.Module):
    """
    3-way Multi-modal YOLO with Attention Fusion
    ì…ë ¥: Thermal, NVG, RGB Enhanced
    ì¶œë ¥: YOLO detection
    """
    
    def __init__(self, 
                 backbone_weights='yolo11x.pt',
                 fusion_dim=512,
                 num_heads=8,
                 dropout=0.1):
        """
        Args:
            backbone_weights: YOLO ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ê²½ë¡œ
            fusion_dim: Fusion layerì˜ feature dimension
            num_heads: Attention heads ìˆ˜
            dropout: Dropout rate
        """
        super().__init__()
        
        # Load YOLO base model
        base_model = YOLO(backbone_weights).model
        
        # Create 3 separate backbones for each modality
        self.thermal_backbone = self._create_backbone(base_model)
        self.nvg_backbone = self._create_backbone(base_model)
        self.rgb_backbone = self._create_backbone(base_model)
        
        # Attention fusion module
        self.fusion = CrossModalAttention(
            dim=fusion_dim,
            num_heads=num_heads,
            dropout=dropout
        )
        
        # Detection head (shared across modalities)
        self.detection_head = base_model.model[-1]  # YOLO head
        
        # Feature dimension adapter (if needed)
        self.feat_adapter = nn.Conv2d(
            in_channels=1024,  # YOLO11x backbone output
            out_channels=fusion_dim,
            kernel_size=1
        )
    
    def _create_backbone(self, base_model):
        """Create a copy of YOLO backbone"""
        import copy
        backbone = copy.deepcopy(base_model.model[:-1])  # Exclude head
        return backbone
    
    def forward(self, thermal, nvg, rgb_enhanced):
        """
        Args:
            thermal: [B, 3, H, W] or [B, 1, H, W]
            nvg: [B, 3, H, W] or [B, 1, H, W]
            rgb_enhanced: [B, 3, H, W]
        
        Returns:
            detections: YOLO output format
        """
        # Extract features from each modality
        feat_thermal = self.thermal_backbone(thermal)
        feat_nvg = self.nvg_backbone(nvg)
        feat_rgb = self.rgb_backbone(rgb_enhanced)
        
        # Get the last feature map (P5 level for YOLO)
        feat_thermal = feat_thermal[-1]  # [B, 1024, H/32, W/32]
        feat_nvg = feat_nvg[-1]
        feat_rgb = feat_rgb[-1]
        
        # Adapt feature dimensions
        feat_thermal = self.feat_adapter(feat_thermal)  # [B, 512, H/32, W/32]
        feat_nvg = self.feat_adapter(feat_nvg)
        feat_rgb = self.feat_adapter(feat_rgb)
        
        # Fusion via Cross-Modal Attention
        fused_feat = self.fusion(feat_thermal, feat_nvg, feat_rgb)
        
        # Detection head
        detections = self.detection_head(fused_feat)
        
        return detections
    
    def load_thermal_weights(self, thermal_model_path):
        """Load pre-trained thermal model weights"""
        thermal_model = YOLO(thermal_model_path).model
        self.thermal_backbone.load_state_dict(
            thermal_model.model[:-1].state_dict()
        )
        print(f"Loaded thermal weights from {thermal_model_path}")
    
    def load_all_backbones_from_thermal(self, thermal_model_path):
        """Initialize all 3 backbones with thermal weights (transfer learning)"""
        thermal_state = YOLO(thermal_model_path).model.model[:-1].state_dict()
        
        self.thermal_backbone.load_state_dict(thermal_state)
        self.nvg_backbone.load_state_dict(thermal_state)
        self.rgb_backbone.load_state_dict(thermal_state)
        
        print(f"Initialized all backbones from {thermal_model_path}")
```

## 2-3. Custom Data Loader êµ¬í˜„

**models/data_loader.py**:
```python
import torch
from torch.utils.data import Dataset, DataLoader
import cv2
import numpy as np
from pathlib import Path
import albumentations as A
from albumentations.pytorch import ToTensorV2

class MultiModalDataset(Dataset):
    """Multi-modal dataset for thermal + nvg + visual_enhanced"""
    
    def __init__(self, 
                 data_root,
                 split='train',
                 img_size=640,
                 augment=True):
        """
        Args:
            data_root: ë°ì´í„° ë£¨íŠ¸ ê²½ë¡œ
            split: 'train', 'val', 'test'
            img_size: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°
            augment: ë°ì´í„° ì¦ê°• ì—¬ë¶€
        """
        self.data_root = Path(data_root)
        self.split = split
        self.img_size = img_size
        self.augment = augment
        
        # Paths
        self.thermal_dir = self.data_root / split / 'thermal'
        self.nvg_dir = self.data_root / split / 'nvg'
        self.visual_dir = self.data_root / split / 'visual_enhanced'
        self.label_dir = self.data_root / split / 'labels'
        
        # Get image list
        self.image_files = sorted(list(self.thermal_dir.glob('*.jpg')))
        
        # Augmentation
        self.transform = self._get_transforms()
    
    def _get_transforms(self):
        """ë°ì´í„° ì¦ê°• íŒŒì´í”„ë¼ì¸"""
        if self.augment and self.split == 'train':
            return A.Compose([
                # ê³µê°„ ë³€í™˜ (3ê°œ ëª¨ë‹¬ë¦¬í‹° ë™ì¼ ì ìš©)
                A.HorizontalFlip(p=0.5),
                A.ShiftScaleRotate(
                    shift_limit=0.1,
                    scale_limit=0.2,
                    rotate_limit=10,
                    p=0.5
                ),
                A.RandomCrop(self.img_size, self.img_size, p=1.0),
                
                # ë°ê¸°/ëŒ€ë¹„ (ëª¨ë‹¬ë¦¬í‹°ë³„ ë…ë¦½ ì ìš©)
                A.RandomBrightnessContrast(p=0.5),
                A.RandomGamma(p=0.3),
                
                # Normalization
                A.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225]),
                ToTensorV2()
            ], additional_targets={'nvg': 'image', 'rgb': 'image'},
               bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))
        else:
            return A.Compose([
                A.Resize(self.img_size, self.img_size),
                A.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225]),
                ToTensorV2()
            ], additional_targets={'nvg': 'image', 'rgb': 'image'},
               bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        # Load images
        img_name = self.image_files[idx].stem
        
        thermal = cv2.imread(str(self.thermal_dir / f'{img_name}.jpg'))
        nvg = cv2.imread(str(self.nvg_dir / f'{img_name}.jpg'))
        rgb = cv2.imread(str(self.rgb_dir / f'{img_name}.jpg'))
        
        thermal = cv2.cvtColor(thermal, cv2.COLOR_BGR2RGB)
        nvg = cv2.cvtColor(nvg, cv2.COLOR_BGR2RGB)
        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)
        
        # Load labels (YOLO format)
        label_file = self.label_dir / f'{img_name}.txt'
        if label_file.exists():
            labels = np.loadtxt(label_file).reshape(-1, 5)
            bboxes = labels[:, 1:5]  # x_center, y_center, w, h
            class_labels = labels[:, 0].astype(int).tolist()
        else:
            bboxes = np.array([])
            class_labels = []
        
        # Apply transforms
        transformed = self.transform(
            image=thermal,
            nvg=nvg,
            rgb=rgb,
            bboxes=bboxes,
            class_labels=class_labels
        )
        
        thermal_tensor = transformed['image']
        nvg_tensor = transformed['nvg']
        rgb_tensor = transformed['rgb']
        bboxes_transformed = np.array(transformed['bboxes'])
        class_labels_transformed = transformed['class_labels']
        
        # Convert to YOLO label format
        if len(bboxes_transformed) > 0:
            labels_tensor = torch.zeros((len(bboxes_transformed), 6))
            labels_tensor[:, 0] = 0  # Batch index (filled by collate_fn)
            labels_tensor[:, 1] = torch.tensor(class_labels_transformed)
            labels_tensor[:, 2:] = torch.tensor(bboxes_transformed)
        else:
            labels_tensor = torch.zeros((0, 6))
        
        return {
            'thermal': thermal_tensor,
            'nvg': nvg_tensor,
            'rgb': rgb_tensor,
            'labels': labels_tensor,
            'img_name': img_name
        }


def collate_fn(batch):
    """Custom collate function for multi-modal batches"""
    thermal_batch = torch.stack([item['thermal'] for item in batch])
    nvg_batch = torch.stack([item['nvg'] for item in batch])
    rgb_batch = torch.stack([item['rgb'] for item in batch])
    
    # Labels with batch index
    labels = []
    for i, item in enumerate(batch):
        l = item['labels']
        l[:, 0] = i  # Set batch index
        labels.append(l)
    labels = torch.cat(labels, 0)
    
    return {
        'thermal': thermal_batch,
        'nvg': nvg_batch,
        'rgb': rgb_batch,
        'labels': labels
    }
```

---

# Phase 3: Multi-modal ëª¨ë¸ Fine-tuning í›ˆë ¨

## 3-1. í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸

**scripts/phase3_multimodal.py**:
```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path
import yaml
from tqdm import tqdm
import numpy as np

import sys
sys.path.append('..')
from models.multimodal_yolo import MultiModalYOLO
from models.data_loader import MultiModalDataset, collate_fn

class MultiModalTrainer:
    def __init__(self, config_path='configs/hyperparams.yaml'):
        # Load config
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        # Device
        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {self.device}")
        
        # Model
        self.model = MultiModalYOLO(
            backbone_weights=self.config['model']['backbone_weights'],
            fusion_dim=self.config['model']['fusion_dim'],
            num_heads=self.config['model']['num_heads'],
            dropout=self.config['model']['dropout']
        ).to(self.device)
        
        # Load Phase 1 thermal weights
        thermal_weights = self.config['pretrain']['thermal_model_path']
        self.model.load_all_backbones_from_thermal(thermal_weights)
        
        # Data loaders
        self.train_loader = self._create_dataloader('train')
        self.val_loader = self._create_dataloader('val')
        
        # Optimizer
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config['training']['lr0'],
            weight_decay=self.config['training']['weight_decay']
        )
        
        # Scheduler
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=self.config['training']['epochs'],
            eta_min=self.config['training']['lr0'] * self.config['training']['lrf']
        )
        
        # Loss function (YOLO loss from Ultralytics)
        from ultralytics.utils.loss import v8DetectionLoss
        self.criterion = v8DetectionLoss(self.model.detection_head)
        
        # AMP Scaler
        self.scaler = torch.cuda.amp.GradScaler(enabled=self.config['training']['amp'])
        
        # Metrics
        self.best_map = 0.0
        self.train_losses = []
        self.val_maps = []
    
    def _create_dataloader(self, split):
        dataset = MultiModalDataset(
            data_root=self.config['data']['root'],
            split=split,
            img_size=self.config['training']['imgsz'],
            augment=(split == 'train')
        )
        
        loader = DataLoader(
            dataset,
            batch_size=self.config['training']['batch'],
            shuffle=(split == 'train'),
            num_workers=self.config['training']['workers'],
            pin_memory=True,
            collate_fn=collate_fn
        )
        
        return loader
    
    def train_one_epoch(self, epoch):
        self.model.train()
        epoch_loss = 0.0
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}')
        for batch_idx, batch in enumerate(pbar):
            # Move to device
            thermal = batch['thermal'].to(self.device)
            nvg = batch['nvg'].to(self.device)
            rgb = batch['rgb'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            # Forward with AMP
            with torch.cuda.amp.autocast(enabled=self.config['training']['amp']):
                predictions = self.model(thermal, nvg, rgb)
                loss = self.criterion(predictions, labels)
            
            # Backward
            self.optimizer.zero_grad()
            self.scaler.scale(loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()
            
            # Log
            epoch_loss += loss.item()
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        avg_loss = epoch_loss / len(self.train_loader)
        self.train_losses.append(avg_loss)
        
        return avg_loss
    
    @torch.no_grad()
    def validate(self, epoch):
        self.model.eval()
        
        # TODO: Implement mAP calculation
        # For now, placeholder
        val_map = 0.0
        
        self.val_maps.append(val_map)
        
        # Save best model
        if val_map > self.best_map:
            self.best_map = val_map
            save_path = Path('weights/multimodal_best.pt')
            save_path.parent.mkdir(exist_ok=True)
            torch.save({
                'epoch': epoch,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'mAP': val_map,
            }, save_path)
            print(f"Best model saved: mAP={val_map:.4f}")
        
        return val_map
    
    def train(self):
        epochs = self.config['training']['epochs']
        
        for epoch in range(epochs):
            # Train
            train_loss = self.train_one_epoch(epoch)
            
            # Validate
            val_map = self.validate(epoch)
            
            # Scheduler step
            self.scheduler.step()
            
            # Log
            print(f"Epoch {epoch+1}/{epochs} - "
                  f"Train Loss: {train_loss:.4f}, "
                  f"Val mAP: {val_map:.4f}, "
                  f"LR: {self.optimizer.param_groups[0]['lr']:.6f}")
            
            # Save checkpoint
            if (epoch + 1) % self.config['training']['save_period'] == 0:
                save_path = Path(f'weights/multimodal_epoch{epoch+1}.pt')
                save_path.parent.mkdir(exist_ok=True)
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                }, save_path)


if __name__ == '__main__':
    trainer = MultiModalTrainer(config_path='configs/hyperparams.yaml')
    trainer.train()
```

## 3-2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • íŒŒì¼

**configs/hyperparams.yaml**:
```yaml
# Model configuration
model:
  backbone_weights: 'yolo11x.pt'
  fusion_dim: 512
  num_heads: 8
  dropout: 0.1

# Data configuration
data:
  root: 'data/nighttime'
  nc: 1
  names: ['person']

# Pre-training
pretrain:
  thermal_model_path: 'weights/thermal_baseline_best.pt'

# Training configuration
training:
  # Basic
  epochs: 100
  batch: 12              # RTX 4090 24GB ê¸°ì¤€ (ì¡°ì • ê°€ëŠ¥)
  imgsz: 640             # ë˜ëŠ” 1280 (ê³ í•´ìƒë„)
  workers: 8
  device: 0
  seed: 42
  
  # Optimizer
  optimizer: 'AdamW'
  lr0: 0.0001            # Fine-tuningì´ë¯€ë¡œ ë‚®ì€ í•™ìŠµë¥ 
  lrf: 0.01              # ìµœì¢… lr = lr0 * lrf
  momentum: 0.937
  weight_decay: 0.0005
  
  # Scheduler
  cos_lr: true
  
  # Performance
  amp: true              # Automatic Mixed Precision
  
  # Saving
  save_period: 10
  
  # Early stopping
  patience: 50

# Validation
validation:
  conf_thres: 0.25       # Confidence threshold
  iou_thres: 0.45        # NMS IoU threshold
```

## 3-3. í›ˆë ¨ ì‹¤í–‰

```bash
# Phase 3 ì‹œì‘
python scripts/phase3_multimodal.py

# ì˜ˆìƒ ì†Œìš” ì‹œê°„ (RTX 4090 ê¸°ì¤€)
# - 100 epochs, 700 images, batch 12
# - ì•½ 4-6ì‹œê°„ (3ê°œ ë°±ë³¸ì´ë¯€ë¡œ Phase 1ë³´ë‹¤ ì˜¤ë˜ ê±¸ë¦¼)
```

---

# Phase 4: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”

## 4-1. ì‹¤í—˜í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°

### ìš°ì„ ìˆœìœ„ ë†’ìŒ
```yaml
# 1. Learning Rate
lr0: [0.0001, 0.0005, 0.001]

# 2. Batch Size
batch: [8, 12, 16]  # ë©”ëª¨ë¦¬ í—ˆìš© ë²”ìœ„ ë‚´

# 3. Image Size
imgsz: [640, 1280]  # ê³ í•´ìƒë„ ì‹¤í—˜

# 4. Fusion Dimension
fusion_dim: [256, 512, 1024]

# 5. Attention Heads
num_heads: [4, 8, 16]
```

### ìš°ì„ ìˆœìœ„ ì¤‘ê°„
```yaml
# 6. Dropout
dropout: [0.0, 0.1, 0.2]

# 7. Weight Decay
weight_decay: [0.0001, 0.0005, 0.001]

# 8. Data Augmentation strength
hsv_v: [0.2, 0.4, 0.6]
degrees: [5, 10, 15]
```

## 4-2. Grid Search ìŠ¤í¬ë¦½íŠ¸

**scripts/hyperparameter_search.py**:
```python
import itertools
import yaml

# ì‹¤í—˜í•  ì¡°í•©
lr_values = [0.0001, 0.0005]
batch_values = [12, 16]
imgsz_values = [640, 1280]

experiments = list(itertools.product(lr_values, batch_values, imgsz_values))

for i, (lr, batch, imgsz) in enumerate(experiments):
    print(f"\n=== Experiment {i+1}/{len(experiments)} ===")
    print(f"lr={lr}, batch={batch}, imgsz={imgsz}")
    
    # Update config
    config = {
        'model': {...},
        'training': {
            'lr0': lr,
            'batch': batch,
            'imgsz': imgsz,
            ...
        }
    }
    
    # Save temporary config
    with open(f'configs/exp_{i}.yaml', 'w') as f:
        yaml.dump(config, f)
    
    # Run training
    # trainer = MultiModalTrainer(config_path=f'configs/exp_{i}.yaml')
    # results = trainer.train()
    
    # Log results
    # ...
```

---

# Phase 5: ìµœì¢… í‰ê°€ ë° ë°°í¬

## 5-1. í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

**scripts/evaluate.py**:
```python
import torch
from pathlib import Path
import cv2
import numpy as np
from models.multimodal_yolo import MultiModalYOLO

def evaluate_model(model_path, test_data_path):
    """ìµœì¢… ëª¨ë¸ í‰ê°€"""
    
    # Load model
    model = MultiModalYOLO()
    checkpoint = torch.load(model_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    model.cuda()
    
    # Metrics
    total_tp = 0
    total_fp = 0
    total_fn = 0
    
    # Test images
    test_dir = Path(test_data_path)
    
    for img_path in test_dir.glob('*.jpg'):
        # Load 3 modalities
        thermal = cv2.imread(str(img_path))  # Load thermal
        nvg = cv2.imread(...)  # Load nvg
        rgb = cv2.imread(...)  # Load rgb_enhanced
        
        # Preprocess
        # ...
        
        # Inference
        with torch.no_grad():
            predictions = model(thermal, nvg, rgb)
        
        # Compute metrics
        # ...
    
    # Calculate mAP
    precision = total_tp / (total_tp + total_fp)
    recall = total_tp / (total_tp + total_fn)
    f1 = 2 * (precision * recall) / (precision + recall)
    
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")
    
    return precision, recall, f1

if __name__ == '__main__':
    evaluate_model(
        model_path='weights/multimodal_best.pt',
        test_data_path='data/nighttime/test'
    )
```

## 5-2. ì¶”ë¡  ë° ì‹œê°í™”

**scripts/inference.py**:
```python
import torch
import cv2
import numpy as np
from models.multimodal_yolo import MultiModalYOLO

def inference_single_image(model, thermal_path, nvg_path, rgb_path):
    """ë‹¨ì¼ ì´ë¯¸ì§€ ì¶”ë¡ """
    
    # Load images
    thermal = cv2.imread(thermal_path)
    nvg = cv2.imread(nvg_path)
    rgb = cv2.imread(rgb_path)
    
    # Preprocess
    # ... (normalize, resize, to_tensor)
    
    # Inference
    model.eval()
    with torch.no_grad():
        predictions = model(thermal, nvg, rgb)
    
    # Post-process (NMS, threshold)
    # ...
    
    # Draw bounding boxes
    for bbox in predictions:
        x1, y1, x2, y2, conf, cls = bbox
        cv2.rectangle(thermal, (x1, y1), (x2, y2), (255, 128, 0), 2)
        cv2.putText(thermal, f'Person {conf:.2f}', 
                   (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 
                   0.5, (255, 128, 0), 2)
    
    # Save result
    cv2.imwrite('output/detection_result.jpg', thermal)
    
    return predictions

if __name__ == '__main__':
    model = MultiModalYOLO()
    model.load_state_dict(torch.load('weights/multimodal_best.pt')['model_state_dict'])
    model.cuda()
    
    predictions = inference_single_image(
        model,
        'test_thermal.jpg',
        'test_nvg.jpg',
        'test_rgb_enhanced.jpg'
    )
```

---

# ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸

## Phase 0: í™˜ê²½ ì„¤ì •
- [ ] Conda í™˜ê²½ ìƒì„±
- [ ] PyTorch + CUDA ì„¤ì¹˜
- [ ] Ultralytics YOLO ì„¤ì¹˜
- [ ] í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
- [ ] ë°ì´í„°ì…‹ ì¤€ë¹„ (YOLO í˜•ì‹)
- [ ] YAML íŒŒì¼ ì‘ì„±

## Phase 1: Baseline
- [ ] thermal.yaml ì„¤ì •
- [ ] Phase 1 í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
- [ ] TensorBoard ëª¨ë‹ˆí„°ë§
- [ ] mAP > 0.70 í™•ì¸
- [ ] Best weights ì €ì¥

## Phase 2: ëª¨ë¸ êµ¬í˜„
- [ ] attention_fusion.py ì‘ì„±
- [ ] multimodal_yolo.py ì‘ì„±
- [ ] data_loader.py ì‘ì„±
- [ ] ëª¨ë¸ êµ¬ì¡° í…ŒìŠ¤íŠ¸ (forward pass)

## Phase 3: Fine-tuning
- [ ] hyperparams.yaml ì„¤ì •
- [ ] Phase 1 ê°€ì¤‘ì¹˜ ë¡œë“œ í™•ì¸
- [ ] Phase 3 í›ˆë ¨ ì‹œì‘
- [ ] Loss ê°ì†Œ ëª¨ë‹ˆí„°ë§
- [ ] Best model ì €ì¥

## Phase 4: ìµœì í™”
- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‹¤í—˜
- [ ] Ablation study (ëª¨ë‹¬ë¦¬í‹° ì œê±° ì‹¤í—˜)
- [ ] ìµœì  ì¡°í•© ì„ íƒ

## Phase 5: í‰ê°€
- [ ] Test set í‰ê°€
- [ ] mAP, Precision, Recall ì¸¡ì •
- [ ] ì‹œê°í™” ê²°ê³¼ í™•ì¸
- [ ] ìµœì¢… ëª¨ë¸ ì €ì¥

---

# ì˜ˆìƒ ì„±ëŠ¥ ëª©í‘œ

## Phase 1 (Baseline)
```
ì—´ìƒ ë‹¨ì¼ ëª¨ë‹¬:
- mAP50: 0.75 ~ 0.85
- mAP50-95: 0.50 ~ 0.65
```

## Phase 3 (Multi-modal)
```
3-way Attention Fusion:
- mAP50: 0.85 ~ 0.92 (Baseline ëŒ€ë¹„ +10%)
- mAP50-95: 0.65 ~ 0.75 (Baseline ëŒ€ë¹„ +15%)
- Precision: 0.85+
- Recall: 0.80+
```

## ì„±ëŠ¥ í–¥ìƒ ê·¼ê±°
1. **ì—´ìƒ**: ì˜¨ë„ ê¸°ë°˜ ì‚¬ëŒ íƒì§€ (ê¸°ë³¸)
2. **ì•¼ê°„íˆ¬ì‹œ**: ê·¼ì ì™¸ì„  ë””í…Œì¼ ì¶”ê°€
3. **Enhanced ê°€ì‹œê´‘**: RGB ìƒ‰ìƒ ì •ë³´ ë³´ì™„
4. **Attention Fusion**: ìƒí™©ë³„ ìµœì  ëª¨ë‹¬ë¦¬í‹° ìë™ ì„ íƒ

---

# ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

## 1. CUDA Out of Memory
```python
# í•´ê²°ì±… 1: Batch size ì¤„ì´ê¸°
batch: 12 â†’ 8

# í•´ê²°ì±… 2: ì´ë¯¸ì§€ í¬ê¸° ì¤„ì´ê¸°
imgsz: 640 (1280 ëŒ€ì‹ )

# í•´ê²°ì±… 3: Gradient Accumulation
accumulate: 2  # Effective batch = batch * accumulate
```

## 2. Lossê°€ ê°ì†Œí•˜ì§€ ì•ŠìŒ
```python
# ì²´í¬ ì‚¬í•­:
1. Learning rate ë„ˆë¬´ ë†’ìŒ â†’ 0.0001ë¡œ ë‚®ì¶”ê¸°
2. ë°ì´í„° ë¡œë” í™•ì¸ (ë¼ë²¨ì´ ì˜¬ë°”ë¥¸ì§€)
3. Phase 1 ê°€ì¤‘ì¹˜ê°€ ì œëŒ€ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
4. Gradient clipping ì¶”ê°€:
   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)
```

## 3. Overfitting
```python
# í•´ê²°ì±…:
1. Dropout ì¦ê°€: 0.1 â†’ 0.2
2. Weight decay ì¦ê°€: 0.0005 â†’ 0.001
3. ë°ì´í„° ì¦ê°• ê°•í™”
4. Early stopping patience ì¤„ì´ê¸°
```

## 4. í•œ ëª¨ë‹¬ë¦¬í‹°ë§Œ í•™ìŠµë¨
```python
# Attention weights ëª¨ë‹ˆí„°ë§
# LearnableModalityWeights ì¶”ê°€í•˜ì—¬ ê° ëª¨ë‹¬ë¦¬í‹° ê¸°ì—¬ë„ í™•ì¸
# í•„ìš”ì‹œ modality-specific loss balancing
```

---

ì´ì œ ì´ ë¬¸ì„œëŒ€ë¡œ ë‹¨ê³„ë³„ë¡œ ì§„í–‰í•˜ì‹œë©´ ë©ë‹ˆë‹¤! ğŸš€
ì¶”ê°€ ì§ˆë¬¸ì´ë‚˜ ë§‰íˆëŠ” ë¶€ë¶„ ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”!