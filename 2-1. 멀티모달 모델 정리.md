# 멀티모달 객체 탐지 모델 훈련 방법론

**목표**: 열상 + 야간투시 + Enhanced 가시광 3-way 입력을 활용한 야간 Person 탐지

---

## 1. 멀티모달 융합 아키텍처 방안

### 방안 1: Early Fusion (초기 융합)
```
[열상] ──┐
[야간투시] ─┼─> Concat/Stack ─> Single Backbone ─> Detection Head ─> Output
[가시광] ──┘
```

**구현**:
- 3개 이미지를 채널 축으로 연결 (3+3+3 = 9채널 또는 1+1+3 = 5채널)
- YOLO 백본의 첫 Conv 레이어를 9채널 입력으로 수정
- 단일 네트워크가 모든 모달리티를 동시에 학습

**장점**:
- 구현 간단, 기존 YOLO 구조 거의 그대로 사용
- 모달리티 간 저수준(low-level) 특징 융합 가능
- 추론 속도 빠름

**단점**:
- 각 모달리티의 독립적 특징 추출 능력 제한
- 한 모달리티가 품질 낮으면 전체 성능 저하
- 사전학습 가중치 활용 어려움 (입력 채널 수 변경)

**코드 예시 (YOLOv8/v11 기반)**:
```python
# 입력 채널 수정
model = YOLO('yolo11x.pt')
# Conv2d(3 -> 9) 첫 레이어 수정
model.model.model[0].conv = nn.Conv2d(9, 64, kernel_size=3, stride=2, padding=1)

# 데이터 로더
def prepare_input(thermal, nvg, rgb_enhanced):
    # 각각 [B, C, H, W]
    return torch.cat([thermal, nvg, rgb_enhanced], dim=1)  # [B, 9, H, W]
```

---

### 방안 2: Late Fusion (후기 융합)
```
[열상] ─────> Backbone A ─┐
[야간투시] ─> Backbone B ─┼─> Fusion Module ─> Detection Head ─> Output
[가시광] ───> Backbone C ─┘
```

**구현**:
- 각 모달리티마다 독립적인 백본 네트워크
- 고수준(high-level) 특징 맵을 융합
- Fusion Module: Concat + Conv, Attention, Weighted Sum 등

**장점**:
- 각 모달리티의 사전학습 모델 활용 가능 (전이학습)
- 한 모달리티 품질 저하 시 다른 모달리티로 보완 가능
- 모달리티별 독립적 특징 추출

**단점**:
- 파라미터 수 3배 증가 (메모리/계산량 증가)
- 추론 속도 느림
- 구현 복잡도 높음

**코드 예시**:
```python
class LateMultimodalYOLO(nn.Module):
    def __init__(self):
        super().__init__()
        self.thermal_backbone = YOLO('yolo11x.pt').model
        self.nvg_backbone = YOLO('yolo11x.pt').model
        self.rgb_backbone = YOLO('yolo11x.pt').model
        
        # Fusion module
        self.fusion = nn.Sequential(
            nn.Conv2d(256*3, 256, 1),  # Feature map concat
            nn.BatchNorm2d(256),
            nn.ReLU()
        )
        
        self.detection_head = DetectionHead()
    
    def forward(self, thermal, nvg, rgb):
        feat_t = self.thermal_backbone.extract_features(thermal)
        feat_n = self.nvg_backbone.extract_features(nvg)
        feat_r = self.rgb_backbone.extract_features(rgb)
        
        fused = self.fusion(torch.cat([feat_t, feat_n, feat_r], dim=1))
        return self.detection_head(fused)
```

---

### 방안 3: Mid Fusion (중간 융합)
```
[열상] ─────> Backbone Layers 1-3 ─┐
[야간투시] ─> Backbone Layers 1-3 ─┼─> Fusion ─> Shared Layers 4-N ─> Head
[가시광] ───> Backbone Layers 1-3 ─┘
```

**구현**:
- 초기 레이어는 모달리티별 독립 (저수준 특징)
- 중간부터 융합 후 공유 레이어 (고수준 특징)

**장점**:
- Early와 Late의 장점 결합
- 파라미터 효율적 (Late보다 적음)
- 적절한 균형

**단점**:
- 융합 시점 결정이 하이퍼파라미터 (실험 필요)
- 구현 복잡도 중간

---

### 방안 4: Attention-based Fusion (추천 ⭐)
```
[열상] ─────> Backbone ─┐
[야간투시] ─> Backbone ─┼─> Cross-Attention ─> Detection Head
[가시광] ───> Backbone ─┘     (동적 가중치)
```

**구현**:
- 각 모달리티별 특징 추출
- Cross-Attention으로 중요한 모달리티 동적 선택
- 상황에 따라 가중치 자동 조절 (예: 열상 신뢰도↑ → 열상 가중치↑)

**장점**:
- 최고 성능 (SOTA 멀티모달 모델들이 사용)
- 모달리티 간 상호작용 학습
- 적응적 융합 (상황별 최적 조합)

**단점**:
- 구현 복잡도 가장 높음
- Transformer 블록 필요 (메모리 추가)

**코드 예시 (간소화)**:
```python
class AttentionFusion(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.cross_attn = nn.MultiheadAttention(dim, num_heads=8)
        self.norm = nn.LayerNorm(dim)
    
    def forward(self, feat_t, feat_n, feat_r):
        # [B, C, H, W] -> [HW, B, C]
        feats = torch.stack([feat_t, feat_n, feat_r], dim=0)  # [3, B, C, H, W]
        
        # Cross-attention
        attended, weights = self.cross_attn(feats, feats, feats)
        fused = self.norm(attended.mean(dim=0))  # [B, C, H, W]
        return fused
```

---

## 2. 훈련 전략

### 전략 A: End-to-End 직접 학습
```
야간 3-way 데이터셋 → 멀티모달 모델 훈련 (처음부터)
```

**방법**:
- 수집한 야간 3-way 데이터셋으로 바로 훈련
- YOLO 구조 수정 → 랜덤 초기화 또는 COCO 사전학습 가중치 일부 활용

**장점**:
- 단순 명료
- 실제 적용 환경과 직접 매칭

**단점**:
- 데이터 부족 시 학습 어려움
- 수렴 느림

---

### 전략 B: 단일 모달 사전학습 → 멀티모달 Fine-tuning (추천 ⭐)
```
1단계: 열상만으로 모델 학습 (Plan C)
2단계: 3-way 입력으로 확장 및 Fine-tuning
```

**방법**:
1. **1단계**: 주간 열상 데이터셋으로 열상 YOLO 모델 먼저 학습
   ```python
   model_thermal = YOLO('yolo11x.pt')
   model_thermal.train(data='thermal.yaml', epochs=100)
   ```

2. **2단계**: 학습된 열상 모델 가중치를 기반으로 멀티모달 모델 초기화
   ```python
   # 열상 백본 가중치 로드
   multimodal_model.thermal_branch.load_state_dict(model_thermal.state_dict())
   # 야간투시/가시광 브랜치도 동일 가중치로 초기화 (전이학습)
   multimodal_model.nvg_branch.load_state_dict(model_thermal.state_dict())
   multimodal_model.rgb_branch.load_state_dict(model_thermal.state_dict())
   
   # Fine-tuning
   multimodal_model.train(data='multimodal.yaml', epochs=50, lr=0.0001)
   ```

**장점**:
- 열상 모델이 이미 Person 탐지 능력 확보
- 멀티모달은 추가 정보 활용법만 학습하면 됨
- 수렴 빠름, 성능 안정적

**단점**:
- 2단계 훈련 필요 (시간 소요)

---

### 전략 C: 점진적 모달리티 추가 (Progressive Training)
```
Step 1: 열상만 (Plan C)
Step 2: 열상 + 야간투시 (Plan B)
Step 3: 열상 + 야간투시 + 가시광 (Plan A)
```

**방법**:
- 단일 → 2-way → 3-way 순서로 점진적 확장
- 각 단계마다 새 모달리티 브랜치 추가 및 Fine-tuning

**장점**:
- 각 모달리티의 기여도를 단계별로 확인 가능
- 안정적 학습 경로
- 문제 발생 시 이전 단계로 롤백 가능

**단점**:
- 훈련 시간 가장 오래 걸림

---

### 전략 D: Knowledge Distillation (지식 증류)
```
Teacher: 개별 단일 모달 모델 3개 (각각 고성능)
Student: 3-way 멀티모달 모델 (효율적)
```

**방법**:
1. 열상, 야간투시, 가시광 각각 단일 모델 훈련 (Teacher)
2. Student 모델이 3개 Teacher의 출력을 모두 모방하도록 학습
   ```python
   loss = detection_loss + distillation_loss
   distillation_loss = KL_div(student_output, teacher_thermal_output) +
                       KL_div(student_output, teacher_nvg_output) +
                       KL_div(student_output, teacher_rgb_output)
   ```

**장점**:
- Student 모델이 3개 모델의 지식 모두 흡수
- 단일 모델로 3개 모델 성능 근접 가능

**단점**:
- Teacher 모델 3개 먼저 학습 필요
- 구현 복잡

---

## 3. 데이터 증강 전략

### 3-1. 모달리티별 증강
```python
# 열상: 온도 기반이므로 색상 왜곡 X, 밝기/대비만
thermal_aug = A.Compose([
    A.RandomBrightnessContrast(p=0.5),
    A.GaussianBlur(p=0.3),
    A.RandomRotate90(p=0.5)
])

# 야간투시: 녹색 단색이므로 밝기 변화
nvg_aug = A.Compose([
    A.RandomGamma(p=0.5),
    A.GaussNoise(p=0.3)
])

# Enhanced 가시광: RGB이므로 일반 증강 가능
rgb_aug = A.Compose([
    A.HueSaturationValue(p=0.5),
    A.RandomBrightnessContrast(p=0.5),
    A.ChannelShuffle(p=0.2)
])
```

### 3-2. 동기화된 공간 증강
```python
# 3개 모달리티에 동일한 공간 변환 적용
spatial_aug = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.ShiftScaleRotate(p=0.5),
    A.RandomCrop(640, 640)
], additional_targets={'nvg': 'image', 'rgb': 'image'})

transformed = spatial_aug(
    image=thermal,  # 기본
    nvg=nvg,        # 추가 타겟 1
    rgb=rgb         # 추가 타겟 2
)
```

---

## 4. Loss Function 설계

### 4-1. 기본 Detection Loss
```python
total_loss = λ1 × box_loss + λ2 × cls_loss + λ3 × obj_loss

# YOLO 기본 Loss
- box_loss: CIoU Loss (bbox 정확도)
- cls_loss: BCE Loss (클래스 분류, Person vs Background)
- obj_loss: BCE Loss (객체 존재 여부)
```

### 4-2. 멀티모달 추가 Loss (선택적)

**Consistency Loss** (모달리티 간 일관성):
```python
# 각 모달리티별로 예측한 bbox가 유사해야 함
consistency_loss = MSE(bbox_thermal, bbox_nvg) + 
                   MSE(bbox_nvg, bbox_rgb) +
                   MSE(bbox_rgb, bbox_thermal)

total_loss += λ4 × consistency_loss
```

**Modality Attention Loss** (중요 모달리티 학습):
```python
# 각 상황에서 어떤 모달리티가 중요한지 학습
# Attention weights의 엔트로피 최소화 (명확한 선택)
entropy_loss = -sum(attention_weights * log(attention_weights))

total_loss += λ5 × entropy_loss
```

---

## 5. 평가 및 실험 계획

### Phase 1: Baseline 구축
```
1. Plan C (열상만) 학습 → mAP 측정 (Baseline)
2. Plan B (열상 + 야간투시) 학습 → mAP 비교
3. Plan A (3-way) 학습 → mAP 비교
```

### Phase 2: 융합 방법 비교
```
동일 데이터셋으로:
- Early Fusion → mAP
- Late Fusion → mAP
- Mid Fusion → mAP
- Attention Fusion → mAP
```

### Phase 3: 최적화
```
최고 성능 모델 선택 후:
- 하이퍼파라미터 튜닝
- 데이터 증강 최적화
- Ablation Study (각 모달리티 제거 실험)
```

---

## 6. 추천 방안 요약

### 🏆 최종 추천 조합

**아키텍처**: **Attention-based Fusion (방안 4)**
- 최고 성능 기대
- 적응적 융합

**훈련 전략**: **단일 모달 사전학습 → Fine-tuning (전략 B)**
- 안정적이고 빠른 수렴
- 열상 모델 먼저 확보

**구현 순서**:
```
1. 열상 단일 모델 학습 (Plan C) ✅ Baseline 확보
2. 2-way 모델 (열상 + 야간투시) 실험
3. 3-way 모델 (+ Enhanced 가시광) 최종
4. Attention Fusion 적용하여 최적화
```

---

## 7. 실제 코드 뼈대 (PyTorch + Ultralytics YOLO)

```python
from ultralytics import YOLO
import torch
import torch.nn as nn

class MultimodalYOLO(nn.Module):
    def __init__(self, backbone='yolo11x.pt'):
        super().__init__()
        
        # 각 모달리티 백본 (사전학습 가중치 활용)
        base_model = YOLO(backbone).model
        self.thermal_backbone = base_model.clone()
        self.nvg_backbone = base_model.clone()
        self.rgb_backbone = base_model.clone()
        
        # Attention Fusion
        self.fusion = CrossModalAttention(dim=512)
        
        # Detection Head (YOLO head 재사용)
        self.detection_head = base_model.head
    
    def forward(self, thermal, nvg, rgb):
        # Feature extraction
        feat_t = self.thermal_backbone.backbone(thermal)
        feat_n = self.nvg_backbone.backbone(nvg)
        feat_r = self.rgb_backbone.backbone(rgb)
        
        # Fusion
        fused_feat = self.fusion(feat_t, feat_n, feat_r)
        
        # Detection
        output = self.detection_head(fused_feat)
        return output

# Training
model = MultimodalYOLO()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

for epoch in range(100):
    for batch in dataloader:
        thermal, nvg, rgb, labels = batch
        
        output = model(thermal, nvg, rgb)
        loss = compute_yolo_loss(output, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

---

이 중에서 어떤 방안이 가장 마음에 드시나요? 아니면 다른 접근법이 궁금하신가요?